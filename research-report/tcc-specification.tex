%%
%% This is file `squelette-rr.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% RR.dtx  (with options: `sample')
%% ********************************************************************
%% Copyright (C) 1997-1999 2004 2006-2011 INRIA/APICS/MARELLE by Jose' Grimm
%% This file may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 2003/12/01 or later.
%% An archive of the software can be found at
%%    ftp://ftp-sop.inria.fr/marelle/RR-INRIA

\documentclass[twoside]{article}
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc} % ou \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % ou \usepackage[OT1]{fontenc}
\usepackage{RR}
\usepackage{hyperref}
\usepackage[all]{hypcap}
%% \def\msquare{\mathord{\scalebox{1.5}[1]{\scalerel*{\Box}{\strut}}}}
%%\usepackage[frenchb]{babel} % optionnel
\RRNo{9355}
%% \RTNo{0703}
%%
%% date de publication du rapport
\RRdate{Juillet 2020}
%%
%% Cas d'une version deux
%% \RRversion{2}
%% date de publication de la version 2
%% \RRdater{November 2008}
%%
\RRauthor{% les auteurs
 % Premier auteur, avec une note
Saalik Hatia%
  % note partag\'ee (optionnelle)
  % \thanks[sfn]{Shared foot note}%
 % \and entre chaque auteur s'il y en a plusieurs
  \and
Marc Shapiro%
 % r\'ef\'erence \`a la note partag\'ee
 % \thanksref{sfn}
 % liste longue pour tests de mise en page
% \and Nicolas Bourbaki\thanks{etc} \and Andr\'e Lichnerowicz
% \and Marcel-Paul Sch\"utzenberger \and Jacques-Louis Lions
}
%% Ceci apparait sur chaque page paire.
\authorhead{Hatia \& Shapiro}
%% titre francais long
\RRtitle{Specification of a Transactionally and Causally-Consistent (TCC) database}
%% English title
\RRetitle{Specification of a Transactionally and Causally-Consistent (TCC) database}
%%
%% \titlehead{Example of RR.sty}
%%
% \RRnote{This is a note}
% \RRnote{This is a second note}
%%
\RRresume{  
Les applications à grande échelle sont généralement construites au-dessus de 
base de données géo-distribuées qui tournent sur de multiples centres de données 
répartis dans le monde.
Les défaillances de réseaux sont inévitables, pourtant pour la majorité des 
services en ligne, la disponibilité est essentielle.
Dans ce contexte, le théorème CAP prouve qu’il est impossible d’être à la fois 
hautement disponible et fournir de la cohérence forte.
Sacrifier la cohérence forte, expose les développeurs a des anomalies complexes 
à gérer.

AntidoteDB est une base de données conçue pour être répartie à travers le monde. 
Avec pour objectif de fournir une haute disponibilité avec le plus haut modèle 
de cohérence possible.
Elle garantit de la cohérence causale transactionnelle (TCC) et des types de 
données convergents (CRDTs).
TCC signifie que : (1) si une mise à jour a eu lieu avant une autre, elles 
seront observées dans le même ordre (cohérence causal), et (2) les mises à jour 
appartenant à une même transaction seront vues simultanément ou pas du tout.

Dans AntidoteDB, la base de données est stockée sous la forme d’un journal 
d’opération. 
Dans l’implémentation actuelle le journal croit sans limite. 
L’objectif principal de ce travail est d’écrire la spécification d’un mécanisme 
de troncature du journal sûr, en effectuant des points de contrôles. 
Cela permettra d’avoir des lectures plus rapides ainsi qu’une récupération de 
données plus rapide.
  }
\RRabstract{
Large-scale application are typically built on top of geo-distributed
databases running on multiple datacenters (DCs) situated around the
globe.
Network failures are unavoidable, but in most internet services,
availability is not negotiable; in this context, the CAP theorem proves
that it is impossible to provide both availability and strong
consistency at the same time.
Sacrificing strong consistency, exposes developers to complex anomalies
that are complex to build against.

AntidoteDB is a database designed for geo-replication.
As it aims to provide high availability with the strongest possible
consistency model, it guarantees Transactional Causal Consistency (TCC)
and supports CRDTs.
TCC means that: (1) if one update happens before another, they will be
observed in the same order (causal consistency), and (2) updates in the
same transaction are observed all-or-nothing.

In AntidoteDB, the database is persisted as a journal of operations.
In the current implementation, the journal grows without bound.
The main objective of this work is to specify a mechanism for
pruning the journal safely, by storing recent checkpoints.
This will enable faster reads and crash recovery.
} 
%%
\RRmotcle{base de données, point de contrôle, journal, spécification, cohérence}
\RRkeyword{database, checkpointing, log, specification, consistency}
%%
\RRprojet{Delys - Sorbonne Universit\'e-LIP6, INRIA}  % cas d'un seul projet
%%\RRprojets{}
%%
%% \URLorraine % pour ceux qui sont \`a l'est
%% \URRennes  % pour ceux qui sont \`a l'ouest
%% \URRhoneAlpes % pour ceux qui sont dans les montagnes
%% \URRocq % pour ceux qui sont au centre de la France
%% \URFuturs % pour ceux qui sont dans le virtuel
%% \URSophia % pour ceux qui sont au Sud.
%%
%% \RCBordeaux % centre de recherche Bordeaux - Sud Ouest
%% \RCLille % centre de recherche Lille Nord Europe
\RCParis % Paris Rocquencourt
%% \RCSaclay % Saclay \^Ile de France
%% \RCGrenoble % Grenoble - Rh\^one-Alpes
%% \RCNancy % Nancy - Grand Est
%% \RCRennes % Rennes - Bretagne Atlantique
%% \RCSophia % Sophia Antipolis M\'editerran\'ee

%%
\begin{document}
\RRNo{9355}
\makeRR   

\tableofcontents
\newpage

\section{Problem Statement}
\label{sec:problem-statement}
The geo-distributed database system Antidote persists its
updates in a log.
To recover after a crash, or to materialize a version of interest,
requires to read and to (re-)execute all the corresponding operations
from the journal from the initial state.
As a database ages, this takes longer and longer.

The aim of this research is to speed this up by adding a checkpointing
mechanism.
Initializing from a recent checkpoint requires to re-execute only
the operations after the checkpoint, which can be much faster.
Once a full-system checkpoint exists, the preceding part of the log can
be truncated.

Journaling, materializing, checkpointing and journal truncation are
well-known mechanisms.
However, their interplay is known to be tricky; this will be even more
challenging in Antidote, because of its complex geo-distributed and
sharded structure, and because of its partially-ordered consistency
model.

The aim of this document is to specify rigorously, and to implement
correctly and efficiently, a safe journaling, materializing,
checkpointing and truncation mechanism for Antidote.
This includes the following partial objectives:
\begin{itemize}
\item
  To specify the structures of interest, i.e., distributed journal,
  materialization cache, and checkpoint store.
\item
  To specify the concepts of consistent cut, snapshot, object version,
  checkpoint, and to identify the consistent cuts that are important for
  correctness and performance.
\item
  To specify rigorously the invariants that link the above structures
  together.
\item
  To identify the actors of interest and their methods: transaction
  coordinator, version cache, journal manager, checkpoint manager, etc.
\item
  To specify the pre-, post-, rely- and guarantee-conditions of these
  methods.
\item
  To provide a reference implementation of these methods.
\item
  To show that the implemented methods satisfy their specification.
\end{itemize}
\section{Background and notation}
\label{sec:background}
The Antidote database is geo-distributed across a number of Data Centers
(DCs).
Each DC is strongly consistent; however, updates can originate
concurrently at any DC under the Transactional Causal Consistency model.

\subsection{Data Centers and shards}
\label{sec:dc-shards}
Antidote supports concurrent updates occurring in geo-distributed,
highly-available DCs; each DC originates its own set of updates.
In turn, a DC is partitioned into non-overlapping storage servers called
shards, coordinated by Transaction Coordinators (\emph{figure \ref{fig:architecture}}).
Thus, the journal is not a single sequential data structure, but is
logically the union of a number of sequential \emph{journal streams},
one per shard per DC\@.
The originating shard is the single writer of a journal stream; all 
other replicas are readers.
Furthermore, a stream originating in some shard in some DC is replicated
to the same shard in all other DCs.%
%
\footnote{
%
  We ignore here the fact that a shard is itself replicated in its DC
  for fault tolerance, because this does not change the fact that each
  journal stream is sequential.
}
%

Zooming into a given shard at a given DC, the former contains a \emph{local}
journal stream that logs the updates to this shard originating from the latter,
and a set of remote journal streams, each one replicating the
updates to this same shard at some other DC\@.
%
\footnote{
  In this document we ignore the possibility of adding or removing additional shards 
  and/or DCs while the system is running. 
}

\begin{figure}[tp]
  \centering
  \includegraphics[width=0.75\textwidth]{figures/datacentres.png}
  \caption{Overall architecture of Antidote}
  \label{fig:architecture}
\end{figure}

\begin{figure}[tp]
  \centering
  \includegraphics[width=1\textwidth]{figures/insidedc.png}
  \caption{\label{fig:insidedc}
  Inside a DC. The Transaction Manager supervises Transaction 
  Coordinators that handle all the client transactions. Every shard has a 
  Cache, a Journal, and three daemons with a specific role. Connected to 
  them is a Checkpoint Daemon that creates checkpoints and stores them 
  into a Checkpoint Store.}
\end{figure}

\subsection{Transactional Causal Consistency}
\label{sec:tcc}

Another source of complexity is the Transactional Causal Consistency
(TCC) model.
Each DC is logically sequential, based on a variant of Snapshot
Isolation.
However, two DCs may update the database concurrently, and updates are
related by the \emph{happened-before} partial order (sometimes called
causal order).

To keep track of happened-before, Antidote uses vector
timestamps with one entry per DC\@.
Every event in the database is tagged by the corresponding vector
timestamp.
A \emph{consistent vector timestamp} (CVT) marks a \emph{transactionally- and
causally-consistent cut}.
This means that, if a CVT contains some update $u$, then it also contains all
updates in the same transaction as $u$, as well as those that
happened-before $u$. 
The snapshot time of a transaction is called its \emph{dependency
CVT}, and its effects are identified by its \emph{commit CVT}.

Recall that a given DC-shard has a local journal stream and one journal
stream per remote DC\@.
Similarly, a CVT has a timestamp per DC\@.
We can now map each entry in a CVT (for some DC) to the prefix of the
journal stream (of the same DC) that happened-before it.
This cut forms a \emph{transactionally- and causally-consistent snapshot}.

\subsection{Journal structure}
\label{sec:journal}

The events that impact the state of the store are persisted in a log, called the
Journal herein. 
The Journal constitutes the ground truth of the database state. 
The Journal is logically composed of a number of sequential streams, each of
which records the events originating from a given shard. 
A DC-shard’s Journal stream is replicated to its counterparts at all other DCs. 

\subsection{Records}
\label{sec:record}

The events recorded in the Journal are called Records.
They contain different types of information. 
It can be system information (such as checkpoint operations), transaction
operations (such as commit message) or operations on an object.
These latter operations are essential to the integrity of the database.
The loss of a single record means that all the objects that are materialized
afterward are incorrect.
A record contains these entries:
\begin{itemize}
  \item \textbf{Log Sequence Number (LSN):} A unique monotonically increasing 
  sequence number
  \item \textbf{Timestamp}: Real-time timestamp 
  \item \textbf{TransactionID:} a transaction identifier 
  \item \textbf{Type:} system operation, transaction operation
\end{itemize}
Depending on the type of the operation the following information is included
in the record:
\begin{itemize}
  \item \textbf{Key:} referred object key
  \item \textbf{Operation:} update on the referred object with its arguments
  \item \textbf{Dependency:} CVT of the snapshot the transaction is reading from
  \item \textbf{ListOfParticipants:} participants in the given transaction
  \item \textbf{CommitTime:} commit time of the transaction
\end{itemize}

These are the different \textit{Types} of records:

\begin{itemize}
\item \textbf{Begin:} system record that marks the beginning of a
transaction. 
It contains a Timestamp, TransactionID, Type, followed by the Dependency.
This record is written once per transaction to each shard
participating.
\item \textbf{Prepare:} system record that is written by shards during the first phase 
of the two-phase commit. 
It contains a Timestamp, TransactionID, Type, followed by the ListOfParticipants.
The Timestamp in this context is the clock proposed by the shard as the commit time.
\item \textbf{Commit:} system record that contains the commit time sent by the coordinator
to the shards in the second phase of the two-phase protocol.
It contains a Timestamp, TransactionID, Type, CommitTime.
The commit time is sent to the shard by the transaction Coordinator.
\item \textbf{Abort:} system record that marks the abortion of a transaction.
It contains a Timestamp, TransactionID, Type, ListOfParticipants.
\item \textbf{Update:} transaction record containing the update operation on a given
object.
It contains a Timestamp, TransactionID, Type, Key, Operation.
\item \textbf{Checkpoint:} system record containing information about a Checkpoint.
It contains a Timestamp, Type and Dependency. 
For each checkpoint one record is written.
\item \textbf{Participant:} system record containing information when a new shard 
participates in the transaction.
It contains a Timestamp, TransactionID, Type.
\end{itemize}


\subsection{Checkpoint store}
\label{sec:checkpoint-store}

In this design we periodically persist materialized versions of objects, called
checkpoints in a store called the Checkpoint Store.  
In the worst case after a crash, all volatile state has been lost, the current
state of an object can be computed by: (1) loading the most recent checkpoint
into memory, (2) reading updates that are more recent than the checkpoint from
the journal, (3) applying those updates to the in-memory state.

A checkpoint created from a causally-consistent cut called Checkpoint Time
(CT) (Section \ref{sec:checkpoint-time}) to ensure that there are no consistency
anomalies in the checkpoint.
Once an object has been checkpointed, any earlier journal record that is part of 
the checkpoint for that object is irrelevant from the perspective of durability. 
Once all the objects in a CT have been checkpointed, then the corresponding
prefix of the journal can be truncated.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Consistent cuts of interest}
\label{sec:tcc-cuts}
The causal ordering of events implies that their vector timestamps are
ordered in the same way:
If Event~1 is causally-before Event~2, their vector timestamps
\emph{vt1} and \emph{vt2} are such that 
\emph{vc1 $<$ vc2}.
Note that the converse is not true in Antidote.
The timestamps of two
concurrent events may be either incomparable or arbitrarily ordered (a
timestamp is a safe approximation of happened-before).

The order between vector timestamps \emph{vt1} and \emph{vt2} is defined
as followed:
\begin{itemize}
\item
  \emph{vt1 = vt2} if every entry of \emph{vt1} is equal to the
  corresponding entry of \emph{vt2}.
  They represent the same event.
\item
  \emph{vt1 $\le$ vt2} if every entry of \emph{vt1} is less or equal to
  the corresponding entry of \emph{vt2}.
\item
  \emph{vt1 $<$ vt2} if \emph{vt1 $\le$ vt2} and \emph{vt1 $\neq$ vt2}.
  Event~1 being causally before Event~2 implies \emph{vt1 $<$ vt2}, but the converse
  is not guaranteed.
\item
  \emph{vt1} is incomparable to \emph{vt2} if \emph{vt1 $\neg\le$ vt2
    $\land$ vt1 $\neg\le$ vt2}.
  There is some entry in \emph{vt1} that is lower than the
  corresponding entry in \emph{vt2}, and vice-versa.
  Events~1 and~2 are necessarily concurrent, but the converse is not true;
  that is, if two events are concurrent, this does not guarantee
  that their vector timestamps are incomparable.
\end{itemize}
A vector timestamp represents a \emph{cut} or time of the data store.
We are interested only in \emph{consistent cut} as they have properties that are 
useful for maintaining information about the database.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \commentaire[Saalik]{The idea here is to start with the checkpoint time and 
% build up from there}

\subsection{Checkpoint Time (CT)}
\label{sec:checkpoint-time}
Every time a checkpoint is created, we persist a checkpoint record in the Journal
(Section \ref{sec:journal}).

The variable \emph{Checkpoint Time} designates the \emph{oldest} available
checkpoint, i.e., lowest \emph{CVT} that includes, for every object, a
checkpoint whose state includes all the update records committed at any time
$\le \mathit{Checkpoint Time}$.
State prior to \emph{CT} cannot be safely recovered.


Initially, \emph{Checkpoint Time = $-\infty$}, implying that the journal
cannot be trimmed.
Note that, while recovering from \emph{CT} is safe, typically recovery will
proceed from the \emph{most recent} available checkpoint for performance
reasons.
To save space, a system typically stores only a few numbers of checkpoints,
preferably only one. 
If the checkpoint store does not support versioning (i.e., each object
has a single version), then there is a single checkpoint, identified by
\emph{Checkpoint Time}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{DC-Wide Causal Safe Point (DCSf)}
\label{sec:dcsf}
Each shard in a DC is replicated to all other DCs.
All updates that originate in some DC are sent asynchronously to the
corresponding shard in other DCs.
Although the shard-to-shard connection is FIFO, the storage state of
different shards in the same DC is not causally consistent.
Without extra care, a transaction that reads from multiple shards might
be unsafe.
To avoid this, a transaction should of a shard is missing updates with
respect to another.

The Cure protocol \cite{rep:pro:sh182} is what ensures the TCC properties.
Cure has two main objectives:
\begin{enumerate}%[(1)]
\item To ensure that transactions in a DC commit atomically, and in a
  total order across all shards of that DC\@.
  It uses the Clock-SI design for this purpose
  \cite{rep:pan:1723}).
\item To ensure that that updates are observed in a causally-consistent
  order within a DC\@.
  To this effect, each shard continuously computes a safe lower bound
  for that DC, called its DC-Wide Causal Safe Point (DCSf).%
\footnote{
  % 
  Called the Global Stable Snapshot (GSS) in the Cure paper
  \cite{rep:pro:sh182}.
}
  The DCSf is a \emph{CVT} across all shards of the DC that is
  \emph{causally safe}, i.e., the corresponding updates, and their causal
  predecessors, have been received and persisted by all shards of this DC.
  States that are above the DCSf are not \emph{visible}.
  A transaction whose snapshot time is not earlier than the DCSf must
  be blocked until the DCSf advances beyond it.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Global Causal Stable Point (GCSt)}
\label{sec:causal-stable-snapshot}

In our new design, we will also leverage the concept of a
\emph{Global Causal Stable point} (GCSt).
A GCSt is a state where all concurrent operations have been received and
resolved.
Formally, any updates that are delivered after the GCSt is computed will
have a higher timestamp than the GCSt
\cite[Definition~5.1]{app:rep:1800}.
To simplify the logic, we assume that successive computations of a GCSt
at some shard are monotonically non-decreasing (this is always
possible).
To compute the GCSt each shard shares its DCSf periodically with their
counterparts in other DCs. 
The vague GCSt is computed as the lower bound of all known DCSf.

State that is earlier than the GCSt, can be stored using its 
sequential representation, avoiding any concurrency-related metadata such as
vector clocks or tombstones.
The transitions between successive GCSt's can be explained as sequential
updates.
This makes the representation simpler and more compact and enable the 
use of any sequential database as a checkpoint store.
For instance, both Add-Wins and Remove-Wins sets reduce to classical
sequential sets, and RGA reduces to a classical list.
We leverage this fact by choosing checkpoint states to be earlier than 
GCSt whenever possible, this makes the sequentially consistent.

One issue with GCSt is that it makes progress only when every single DC
is available.
It stops advancing as soon as any single DC does not regularly communicate its
metadata.

\subsection{Min-Dependency and Max-Committed}
\label{sec:dep-commit}
In order to satisfy the properties of checkpoints, of the DCSf and the GCSt we keep
track of all the dependencies of running transactions but also all the commit
times of finished transactions. 

Among those we single out those who are the most important.

Min-Dependency represents the oldest snapshot any
running transaction is reading from. 
Because a checkpoints is sequentially consistent, there should be 
no in-flight transactions at Checkpoint-Time.
To ensure that this property holds true Min-Dependency is used to track the point
beyond which sequentiality is not guaranteed.
When a transaction is finished, committed or aborted, the Min-Dependency
advances to the next Dependency. 
One issue is that while a transaction is running, Min-Dependency
will not advance, and consequently checkpointing will be paused.

Max-Committed is the last commit time recorded in the Journal.
DCSf's advancement is bounded by Max-Committed, making sure that a new
transaction does not read unsafe updates.
Every time a transaction commits, its commit time become the new Max-Committed.
Similarly to Min-Dependency, if no transaction commits, Max-Committed does
not advances, nor does DCSf.

\subsection{Low-Watermark and High-Watermark}
\label{sec:low-high}
To represent the persistent portion of the log we use Low-Watermark and High-Watermark. 
With \emph{Low-Watermark} representing the lower bound and \emph{High-Watermark} 
the higher bound.
Records that precedes \emph{Low-Watermark} may be deleted and records that postdates
\emph{High-Watermark} might be volatile.

\subsection{Invariants}
\label{sec:cuts-invariants}

The overarching goal of this work is to have no perceivable loss of information,
meaning that records that have not been checkpointed must not be deleted.
Hence, our first invariant is $\mathit{Low} \le \mathit{Checkpoint Time}$.
Checkpoints should be sequentially consistent and stable across all DCs.
Hence, the invariants $\mathit{Checkpoint Time} \le
\mathit{GCSt}$ and $\mathit{Checkpoint Time} \le \textit{Min-Dependency}$.
By construction the GCSt is computed as the lower bound of all known DCSfs across
so this gives us the following invariant $\mathit{GCSt} \le \mathit{DCSf}$. 
DCSf represents the point of safety in a DC using shared information between
shards about their registered \emph{commit times} therefore $\mathit{DCSf} \le
\textit{Max-Committed}$. 
These relations control the behavior of each shard (\textit{figure \ref{fig:system-vts}}).

\begin{figure}[tp]
  \centering
  \includegraphics[width=\textwidth]{figures/cvts.png}
  \caption{%
    Relevant system states and their relations (for a given
    shard, at a given DC).
    Each horizontal tape (one per DC) depicts a journal stream for
    this shard.
    Each vertical line depicts a cut of interest and its
    vector timestamp.
    Over time, each journal stream grows to the right (and is
    trimmed to the left), and each state of interest advances
    monotonically to the right.
    As this happens, the causal precedence invariants, denoted
    $\le$, must be maintained.
    To enforce $\le$ requires synchronization between the corresponding processes.
  }
  \label{fig:system-vts}
\end{figure}

At the Journal level there are several invariants that are necessary to ensure that
no data is lost.
First if a Checkpoint exist there should be a \textit{Checkpoint} record present in
the Journal.
All records from a non-checkpointed transaction must have all records 
present in the journal.

\section{Actors}
\subsection{Transaction Manager}
\label{sec:transaction-daemon}

Each DC has a process called the Transaction Manager, which 
receives transactions from clients.
When a client's transaction is received at a DC, it creates a Transaction
Coordinator to manage it.
Every Transaction Coordinator generated is supervised by the Transaction Manager. 
It ensures that they handle the transaction and terminate correctly.
If a Transaction Coordinator crashes before sending a termination 
signal the Transaction Manager creates another Transaction 
Coordinator to recover the transaction.


\subsection{Transaction Coordinator}
\label{sec:transaction-coordinator}
            
A client transaction is managed by a single process called its
Transaction Coordinator, running on some arbitrary server of the DC
where the client is located.
The role of the Coordinator is to start the transaction at each involved
shard, to send each client operation to the correct shard, and to
coordinate the two-phase commit of the transaction among all the shards.

A Transaction Coordinator assigns it a unique transaction ID and
a \emph{Dependency CVT}.
When the transaction issues an operation on some object, the coordinator
forwards it to the appropriate shard in the same DC.

\subsection{Checkpoint Daemon}
\label{sec:checkpoint-daemon}

The Checkpoint Daemon periodically gets materialized versions from the cache and 
persist them in the Checkpoint Store.
The eviction daemon should signal its intent to evict to the
Checkpoint Daemon, enabling the latter to checkpoint 
opportunistically before the eviction occurs.
Furthermore, when the trimming daemon wants to advance, it signals the
Checkpoint Daemon to make progress.
Because we want the Checkpoint Daemon to be sequentially consistent
it may not advance \emph{Checkpoint Time} beyond Min-Dependency.

If the Checkpoint Daemon conflicts in this way with a running
transaction, the daemon must wait for the transaction to terminate,
possibly by asking the Transaction Coordinator to forcibly abort it.


\subsection{Trim Daemon}
\label{sec:trim-daemon}

The Trim Daemon's role is to truncate the Journal. 
It gets a signal from the Checkpoint Daemon when it has finished a checkpoint.
Upon reception of the signal the Trim Daemon scans the Journal to find 
the first begin record of a transaction that is not part of the last transaction.
Low-Watermark is set with the Log Sequence Number of this record.

\subsection{Fill Daemon}
\label{sec:fill-daemon}

The Fill Daemon ensures that the objects that are needed in the cache are properly
materialized. 
When an object version is missing, the Cache sends a signal to the Fill Daemon so it 
can retrieve the needed information in order to build the object.

In order to build an object, the Fill Daemon looks if a version
already exist in the cache. 
If none, then the Fill Daemon retrieves the last checkpointed versions 
from the Checkpoint Store.
If there is no information about the key in the Checkpoint Store, it returns
an empty object.
The Fill Daemon then reads all the operations from the journal that have been made and committed
since.

% Once finished, the Fill Daemon sends a signal to the Checkpoint Daemon or the 
% Transaction Manager to indicate its completion.

If the Fill Daemon cannot fill the cache because the \emph{occupancy}
is full, then it sends a signal to the Evict Daemon~\ref{sec:evict-daemon}.

\subsection{Evict Daemon}
\label{sec:evict-daemon}
 
The Evict Daemon is triggered by either a system policy, or the Fill Daemon, in 
order to free up space. 
If it is triggered by a system policy, then it sends a signal to the 
checkpoint daemon so that objects in the cache can be opportunistically
checkpointed if needed.


\section{Cache}
\label{sec:cache}

The cache is a data structure of materialized versions of objects that are 
currently in use. 
An entry in the cache is identified by a pair \emph{(k,c)} where
\emph{key} is the key of the object, and \emph{c} is the commit timestamp of the
version. 
Each shard maintains its own cache managed by a cache daemon.

\subsection{Object-version descriptors}
\label{sec:objects}

The system maintains the following information about an object's version
in an object-version:
\begin{itemize}
\item
  \emph{key (abbreviated k)} is the unique identifier of the object.
\item
  \emph{commit (abbreviated c)} is the commit vector timestamp of the transaction
  this object is materialized from.
  An uninitialized object-version has a vector timestamp of $-\infty$.
\item
\emph{presence (abbreviated p)}: the presence bit, it is true when the descriptor is
significant. 
Otherwise, it must be ignored.
\item
  \emph{valid (abbreviated v)} is a vector timestamp for which the object has
  not been updated.
  In other words, it is known that there exist no committed
  updates between \emph{commit} and \emph{valid}.
  Equivalently, this version is part of all snapshots between these two
  values.
\item
  \emph{used (abbreviated z)} a bit that indicates that this version has been recently been
  used; useful for cache management.
\item
  \emph{blob (abbreviated b)} is a pointer to the materialized value of the object (if
  any), either in memory or on checkpoint store.
  The system does not interpret the blob, which is managed by the
  object's type.%
  % 
  \footnote{
    % 
    In Antidote, the type is itself encoded as part of the object's key.
    Antidote supports CRDT types such as Counter, LWW-register or
    AW-set, and quasi-CRDTs such as Bounded Counter.
  }
  % 
\end{itemize}

% The couple $(\mathit{k}, \mathit{c})$ is the primary key for an
% object-version.

\subsection{Functions}
The cache has four main functions:
\begin{itemize}
  \item \emph{get(key k, vts dep)} returns the object \emph{k} from the snapshot 
  \emph{dep}. This is called by application and by the checkpoint daemon.
  \item \emph{load(key k, vts c, blob b)} Loads an object k from the checkpoint
  store, and assigns it a c as timestamps. Called by cache daemon.
  \item \emph{inc(key k, vts c, vts t)} apply operations committed on object k from
  between time c and t. Called by cache daemon.
  \item \emph{evict(key k, vts c)} evicts object k timestamped c from the cache. 
  Called by cache daemon.
\end{itemize}

A cache entry (\emph{e}) is represented by its primary key (\emph{key k, vts c}).

\subsection{Reading}
\label{sec:reading}

The main application API is \emph{get}, which accesses the materialized
version of object with key \emph{k} at a snapshot with dependency vector
\emph{dep}.
Its precondition requires that there exists a suitable object-version
descriptor \emph{e} in the cache.%
%
\footnote{
%
  A cache entry is named by its primary key \emph{(key k,
  commit\_vts c)}.
}
%
It must have the same key \emph{k}, and a commit time \emph{c} that
satisfies the dependency, i.e., such that \emph{c $\le$ dep} and there
are no invalidating updates \emph{dep $\le$ v}, where \emph{v} is
\emph{e}'s validity timestamp.
The cache entry must be significant (presence bit \emph{p}) and this
version must be \emph{visible} to the client.

The assertion \emph{visible} abstracts the visibility conditions.
A version is visible to the client that wrote it (read-my-writes).
If the client is not writing the transaction, the version must be
committed and causally stable to ensure all distant updates have been
received.
Furthermore, \emph{visible} requires that access control conditions (not
addressed in this document) are satisfied \cite{sec:rep:1786}.

If the precondition is not satisfied, \emph{get} may either fail, or (in
the case of a cache miss) block and wait.
To this effect, it might send a signal to the cache-filling daemon
discussed next.

When the precondition is satisfied, \emph{get} returns the object and
sets the used bit \emph{z} to true.

The method relies on the environment not removing or invalidating the
cache entry, and not resetting the used bit.
Thus, a rely condition forbids the checkpointing daemon from removing old
checkpoints too aggressively, as \emph{checkpointed}, the time of the
oldest checkpoint, may not advance further than the \emph{dep} of any
executing transaction.

The guarantee clause guarantees to modify no more than the used bit of cache entry
\emph{e}.

\subsection{Filling the cache}
\label{sec:cache-loading}

\emph{Load} allocates and initializes a suitable descriptor \emph{e} and
increments \emph{occupancy}.
If the checkpoint store records the validity timestamp, initialize it accordingly;
otherwise we initialize it to be equal to a checkpoint timestamp.

The guarantee clause of \emph{load} guarantees to not update memory outside of 
\emph{e}.
Its rely clause assumes the environment does not overflow the cache nor update
\emph{e} (other than the used bit).
This may require synchronization with \emph{inc}.

The \emph{inc} method updates an existing cache entry \emph{e = (k,c)}
with any updates from the journal more recent than \emph{c} up to some
time \emph{t}.
The result is a cache entry \emph{e' = (k, c')}, with the same key and a
larger commit time.
The descriptor \emph{e'} may be either the same as \emph{e} (overwrite),
or a newly-allocated one.

The precondition blocks until the journal has caught up with \emph{t},
and requires that \emph{e} was previously \emph{load}ed, since \emph{e}
must exist with its presence bit true, i.e that,
the method initializes \emph{e'} by applying (in causal order and
respecting transaction atomicity) all journal records up to \emph{t}
that are either concurrent to, or later than \emph{e}.
The resulting commit time is the least upper bound of these updates, and
the validity timestamp is \emph{t}.
If \emph{e'} was a new entry, the method increments the occupancy.

Filling the cache is handled by a daemon called Fill Daemon explained later 
in this document (\emph{see \ref{sec:fill-daemon}}).

\subsection{Eviction}
\label{sec:eviction}

The \emph{evict} method invalidates some entry \emph{e} that is present
and not used; in practice, it will be triggered either periodically, or
by a signal from the Fill Daemon.
Importantly, if \emph{get} sets the \emph{used} bit during its
execution, the method exits with no effect.
Otherwise, the method simply sets the presence bit to false and
decreases the occupancy.

The rely clause state that the environment does not modify \emph{e}, except for 
evicting, or setting the \emph{used} bit.
The guarantee clause guarantees that, if the \emph{used} bit changes, the method has 
no effect; otherwise, it modifies only the presence bit.

Eviction is handled by a daemon called Evict Daemon, discussed later in this
document\ref{sec:evict-daemon}.

\section{Recovery}
\label{sec:recovery}

After a restart, due to a scheduled restart or a crash, the database needs to 
reconstruct a stable view of the database. 
The Transaction Manager is the first process to start, it initializes each shard.
Each shards then starts a recovery algorithm.
The journal in some shard might have incomplete information 
about transactions. 
The goal of the recovery is to reconstruct the information
that is necessary for the database to run correctly and ensure that Journal 
invariants are true.

% The Journals contains information from transaction that are committed or were running when the database stopped, alongside those transaction records there are the record abouts

When recovery is complete the following is true at each shard:
\begin{itemize}
  \item Time of the last checkpoint is known
  \item Low Watermark and High Watermark are set
  \item Every Transaction that has a prepare record in each participating shard also
  has a commit record in each corresponding Journal.
  \item Every Transaction that is missing a prepare record has an abort message 
  written in each corresponding Journal.
\end{itemize}

Recovery proceeds as follows. 
First the Journal is read to find the last checkpoint record that has been written
to the log and set the Checkpoint Time.
Using the Checkpoint Time, and the fact that every committed transaction that is 
not part of the last checkpoint must be complete in Journal.
Low Watermark is initialized with the Log Sequence Number (LSN) of the first 
\textit{Begin} record that comes after the Checkpoint Time. 
% This invariant needs to be added to the journal invariants
% The journal always contains a Checkpoint record if one exist
The High Watermark is then initialized with the LSN of the last complete 
record in the journal with no gaps.

Next, recovery scans the Journal for transactions that have a record in the
journal between Low Watermark and High Watermark, and put their IDs in a set
called \textit{transactionIDs}.
Then two additional sets of transaction IDs, one for transactions with
a commit record, called \textit{committed}, and the second for transactions with
a prepare record, called \textit{prepared}.

Using these three sets a check of the invariant $ committed 
\subseteq prepared \subseteq \emph{transactionIDs} $ is performed.
Should this test fail the recovery will fail.
 
For every transaction that has no commit record or prepare record, recovery adds an 
abort record in the journal in every participating shard. 

For every transaction that has a prepare record but no commit record recovery has 
to re-run two-phase commit.
It sends its ID to the Transaction Manager. 
The Transaction Manager verifies that all participating shard has a
prepare record, and that none has an abort record.
If true, and none have a commit record, then two-phase commit is 
initiated, otherwise the commit record is propagated to the other shards. 
If not an abort record is sent to every participating shards.

Once all shards have complete journals, the database is ready to operate.


% Using the information inside the journal the recovery construct essential information 
% that is needed.
% It also recover information about transactions that were running and did not commit.


% \section{Recovery}
% \label{sec:recovery}
% Upon recovery after a crash, the system must be able to recover the values of
% \emph{Low, High} and \emph{Checkpoint Time}, as well as
% all records between the high and low watermarks, and the checkpoint at
% \emph{Checkpoint Time}.


\section{Current and future work}
\label{sec:current-future}

While the specification of the concepts of the database are taking shape, the
communication between DCs needs to be specified as well.

Once recovery semantics have been set, specifying the lifecycle of the
database will allow us to verify that there is no obvious hole in our design.
The next step will be writing the related pseudo-code and formally verify
it. 
And finally implement and validate through experimentation.

Once all this work is done, there are a few enhancements the design would benefit
from.
In the current design all the DCs are sharded identically. 
If one DC was to add another shard, this change should be reflected on other DCs,
considering communication is shard-to-shard and the protocol is designed to
work between identical shards.
Allowing DCs to scale asymmetrically would be a welcomed improvement as DCs
might not have the same workload.

Another objective would be to be able to use multiple storage backends,
including legacy databases or file systems without changing their
native format. 
Antidote would become the transactional and replication layer adding support
for TCC on top.

% \commentaire[Saalik]{vrac}

% Current
% Finish the specification, write recovery, implement a working prototype.

% Future work would be to allow: in DC shard replicating, add new shard, add new
% DCs, allow multiple backends.

\section{Conclusion}
\label{sec:conclusion}

This report presents the work on a design of a database that enables safe 
truncation of the Journal without any observable loss information. 
It combines operations and checkpointed states to reduce the response time
associated with having only operations.
Truncating the Journal while checkpointing during normal operation of the
database is tricky, and if not properly specified, can lead to data loss.

A secondary goal of this design is to create a blueprint for a TCC database that
supports storing states alongside the Journal of operation.


\bibliographystyle{plain}
\bibliography{predef,shapiro-bib-ext,shapiro-bib-perso}


\end{document}
\endinput
%%
%% End of file `squelette-rr.tex'.
